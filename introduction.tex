% \addbibresource{refs.bib}

\section{Introduction} \label{section:introduction}

In our lives, there are many emotional and memorable moments that worth keeping and sharing with others. Therefore, services allowing users to upload and share personal photos are always one of many notable products of different companies such as Facebook, Flickr, Instagram, and Google Photos. This shows that sharing photos is one of greatest demand of users on the Internet. These services also allow users to attach some memos to their photos as well as to search their photos more easily using text queries.

Currently, the most common way for user to do so is to tag their photos manually which takes users a lot of time and effort. There are also some proposed methods (cite) and smart systems which are able to automatically identify noticeable landmarks or location related to the photos such as Google Photos and Flickr. However, these automated annotation are identical for all users and thus do not reflect one's own memories, feelings or characteristics. For example, these systems would recommend phrases like "Eiffel Tower", "a dog", or "a cat" rather than "where I first met my lover" or the name of your pet. Therefore, it owuld be necessary for a system to automatically tag users' photos with personalized caption with respect to their personal features.

In this paper, we propose to develop a system that can suggest appropriate annotations for each photo uploaded by users using Visual Instance Search. In our system users can assign his/her personalized annotations for some photos as initial examples then, the system will automatically propagate these annotations to other existed photos in their collection based on the visual similarities among photos. For each uploaded photo, the system will base on the visual similarity between the uploaeded photo and already-annotated photos of the corresponding users to identify a list of suitable annotations for the uploaded photo in the descending order of the similarity. Then, users can choose to approve reasonable annotation for the uploaded photo. Inaddition, if a user upload more than one photos and change the annotations, the system will have more samples to reference from and thus, it will be better adapted to the user's interests. Therefore, our system is not only able to recommend proper annotations which are unique for each user but also to interactively learn and adapt as users change the annotations.

Since the problem of retrieving similar images in a collection corresponding to a single image has been developed for years, there are many different approaches that can be applied to solve this problem. One of them is template matching method, i.e. a technique for finding small parts of an image which match a template image \cite{brunelli_template_matching}, \cite{Rosenfeld4309663, Gharavi913587}. Another popular technique is to evaluate the similarity of two images by comparing some regions which seem seem to be the interested point of the images, namely features matching \cite{Belongie710790, Rubner, Viola990517}. In our system, the authors develop our own Visual Instance Search framework using Bag-of-Words model. In Bag-of-Words model, each image is represented as a histograms of pre-trained visual words (codebook). Since Bag-of-Words allows parts of a query image to appear flexible in the result images, it is a potential approach that is widely used and and focused by many research groups. 

Together with the exponential increasing of the number of uploaded images, the system faces lots of difficulty adapting those new images. Since re-training the codebook requires changing Bag-of-Words vector of users' existing images and takes too much time, the authors propose to use a fixed codebook trained with different types of images (e.g. cars, dogs, cats, buildings...) and use it universally. Because of the varieties of those different images, it is appropriate to compute and represent any new images' Bag-of-Words vector without changing the codebook. We trained our codebook using ABC dataset tested our system on XYZ dataset. Our performed experiments show that ...

Our main contributions in this paper are as follows:
\begin{itemize}
	\item First, we propose the idea and realize the system that can recommend annotation for photos with visual instance search.
	\item Second, our system allows recommended annotation is personalized and varies from user to user.
	\item Third, our system is interactively user adaptive, i.e. the more a user annotates his/her photos via our system, the more accurate the recommended annotations are.
\end{itemize}

The rest of this paper is organized as follows. In section \ref{section:background_relatedworks}, we review the background and related words in image retrieval and image classification. The core steps of the BoW model and how we conduct experiments are presented in section \ref{section:method}. Section IV shows experiment results and evaluations. The conclusion and future works are discussed in section V.
