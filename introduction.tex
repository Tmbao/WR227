% \addbibresource{refs.bib}

\section{Introduction} \label{section:introduction}

In our lives, there are many emotional and memorable moments that worth keeping and sharing with others. Therefore, services allowing users to upload and share their personal photos are always ones of many notable products of different companies such as Facebook, Flickr, Instagram, and Google Photos. This shows that sharing photos is one of greatest demands of users on the Internet. 

Online photo services usually allow users to attach some memos to their photos as well as to search their photos more easily using text queries. Currently, the most common way for users to do so is to tag their photos manually which consumes a lot of time and effort. There are also some proposed methods \cite{icml2013_chen13e,Lan_2013_CVPR} and smart systems which are able to automatically identify noticeable landmarks or locations related to the photos such as Google Photos and Flickr. However, these automated annotation systems suggest tags that are identical for all users and thus do not reflect one's own memories, feelings, or characteristics. For example, these systems would recommend phrases like "Eiffel Tower", "a dog", or "a cat" rather than "where I first met my lover" or the name of your pet. Therefore, it is necessary to automatically tag users' photos with personalized captions corresponding to their memory and personal characteristics.

In this paper, we propose a system that can suggest appropriate annotations for each photo uploaded by users using Visual Instance Search. In our system, users can assign their personalized annotations for some photos as initial examples, then, the system automatically propagates these annotations to other existed photos in their collection based on visual similarities among the photos. For each uploaded photo, the system bases on visual similarities between that photo and already-annotated photos of the corresponding user to propose a list of suitable annotations for the new uploaded photo in descending order of similarity. Then, the user can choose to approve reasonable annotation for the uploaded photo. In addition, if a user uploads more than one photo and change the annotations, the system has more samples for reference and thus, it tends to better adapt to user's interests. As a result, our system is not only able to recommend proper annotations which are unique for each user but also to interactively and incrementally learn and adapt as users change annotations.

Since the problem of retrieving similar images in a collection corresponding to a single image has been developed for years, there are many different approaches to the problem. One of them is template matching method, i.e. a technique for finding small parts of an image which match a template image \cite{brunelli_template_matching,Rosenfeld4309663,Gharavi913587}. Another popular technique is to evaluate the similarity of two images by comparing some regions which appear to be critical parts of the images, namely features matching \cite{Belongie710790,Rubner,Viola990517}. In this paper, the authors develop our own Visual Instance Search framework using Bag-of-Words (BoW) model. In Bag-of-Words model, each image is represented as a histogram of pre-trained visual words (codebook). Since Bag-of-Words allows parts of a query image to appear in a flexible way in the result images, it is a potential approach that is widely used in many Visual Search systems. 

Together with the exponential increasing of the number of uploaded images, the system faces lots of difficulty adapting those new images. Since re-training the codebook requires changing Bag-of-Words vectors of users' existing images and is also computationally expensive, the authors propose to use a fixed codebook trained with different types of features (e.g. vehicles, animals, buildings...) and use it universally. Because of the varieties of those different features, it is appropriate to compute and represent any new images' Bag-of-Words vectors without changing the codebook. Therefore, we train our codebook on Oxford Building Dataset and use this codebook for our system.

Our main contributions in this paper are as follows:
\begin{itemize}
	\item We propose the idea and realize the system that can recommend annotation for photos with visual instance search.
	\item Our system allows recommended annotation to be personalized and to vary from user to user.
	\item Our system is interactively user adaptive, i.e. the more a user annotates his/her photos via our system, the more accurate the recommended annotations are.
\end{itemize}

The rest of this paper is organized as follows. In section \ref{section:background_relatedworks}, we review the background and related works in image retrieval and image classification. Detailed steps of the automatic annotation system and how we use the BoW model is described in section \ref{section:proposed_system}. Section \ref{section:experiment_result} contains our experiment result. Conclusion is presented in \ref{section:conclusion}

