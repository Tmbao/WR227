% \addbibresource{refs.bib}

\section{Background \& Related works} \label{section:background_relatedworks}
There are many approaches to build an Image Information Retrieval System. Some methods aim at high precision, i.e achieve high quality of top retrieved results, while others focus on high recall, i.e retrieve all positive results. Among them, the first effective and scalable method is Bag-of-Words, Sivic and Zisserman \cite{3}, which is inspired by the correspondence algorithm using in text retrieval. Before going into details of BoW model in subsection \ref{section:background_bow}, we will first introduce some different methods for image retrieval problem in subsection \ref{section:background_dif_method}.

\subsection{Different approaches for image retrieval problem} \label{section:background_dif_method}
One of many popular methods is histogram comparisons which compares 2 different images based on their color histograms. Some early works of this approach using a cross-bin matching cost for histogram comparison can be found in \cite{Shen1983187,Werman1985328,Peleg192468}. In \cite{Peleg192468}, Peleg et al. represent images as sets of pebbles after normalization. The similarity score is then computed as the matching cost of two sets of pebbles based on their distances.

Another well-known technique is template matching, i.e. seeking a given pattern in a image by comparing to candidate regions of the same size in the target image. By consider both the pattern and candidate regions as a length-$N$ vector, we can compare these two vectors by using different kinds of distance metrics, one such metric is the Minkowski distance \cite{Ouyang5770267}. The major disadvantage of 2 listed methods is that they require the query and target images to share a similar stationary interrelation, which means that components of the given image are not allowed to change freely in a certain extent. Bag-of-Words, the method that is discussed in this paper, is another approach that can tolerate the flexibility in structures of the object and thus, have a wider variation of application in many problems. 

\subsection{Bag-of-Words}\label{section:background_bow}

Since Bag-of-Words is originally a text retrieval algorithm, we will first introduce some backgrounds about BoW in text retrieval problem in subsection \ref{section:bow_text} before discussing using BoW in image retrieval in subsection \ref{section:bow_image}.

\subsubsection{Bag-of-Words in text retrieval}\label{section:bow_text}
In text retrieval, a text is represented as a histogram of words, also known as BoW \cite{4}. This scheme is called term frequency weighting as the value of each histogram bin is equal to the number of times the word appears in the document.
Moreover, some words are less informative than others since those words appear in almost every document. Therefore, we need a weighting scheme that address this problem. Such weighting scheme is called inverse document frequency (idf) and is formulated as $log(N_{D} / N_{i})$, where $N_{D}$ is the number of documents in the collection and $N_{i}$ is the number of documents which contains word $i$. The overall BoW representation is thus weighted by multiplying the term frequency (tf) with the inverse document frequency (idf) giving rise to the tf-idf weighting \cite{4}. In addtion, extremely frequent words, ``stop words'', can be removed entirely in order to reduce storage requirements and query time.


\subsubsection{Bag-of-Words in image retrieval} \label{section:bow_image}

When applying BoW to image retrieval, a major obstacle is the fact that text documents are naturally broken into words by spaces, dots, hyphens, or commas. In contrast, there is no such separator in images. Therefore, the concept of ``visual word'' is introduced where each visual word is represented as a cluster obtained using k-means on the local descriptor vectors \cite{3}.

The bigger the vocabulary size is, the more different the visual words are. Hence, the vocabulary helps us distinguish the images more effectively. Nonetheless, with bigger vocabulary size, slightly different descriptors can be assigned to different visual words thus not contributing to the similarity of the respective images and causing a drop in performance examined in \cite{5,6,7}. Philbin et al. \cite{7} suggests ``soft assign'' method where each descriptor is assigned to multiple nearest visual words instead of using ``hard assignment'', i.e only assign a local descriptor to only one nearest visual word. Despite its effectiveness, this method also significantly costs more storage and time.


