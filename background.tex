% \addbibresource{refs.bib}

\section{Background \& Related works} \label{section:background_relatedworks}
There are many approaches to build an Image Information Retrieval System. Some methods aim at high precision, i.e achieve high quality of top retrieved results, while others focus on high recall, i.e retrieve all positive results. Among them, the first effective and scalable method is Bag-of-Words, Sivic and Zisserman \cite{3}, which is inspired by the correspondence algorithm using in text retrieval. Before going into details of BoW model in subsection \ref{section:background_bow}, we will first introduce some different methods for image retrieval problem in subsection \ref{section:background_dif_method}.

\subsection{Different approaches for image retrieval problem} \label{section:background_dif_method}
One of many popular methods is histogram comparisons which compares 2 different images based on their color histograms. Some early works of this approach using a cross-bin matching cost for histogram comparison can be found in \cite{Shen1983187, Werman1985328, Peleg192468}. In \cite{Peleg192468}, Peleg et al. represent images as sets of pebbles after normalization. The similarity score is then computed as the matching cost of two sets of pebbles based on their distances.

Another well-known technique is template matching, i.e. seeking a given pattern in a image by comparing to candidate regions of the same size in the target image. By consider both the pattern and candidate regions as a length-$N$ vector, we can compare these two vectors by using different kinds of distance metrics, one such metric is the Minkowski distance \cite{Ouyang5770267}. The major disadvantage of 2 listed methods is that they require the query and target images to share a similar stationary interrelation, which means that components of the given image are not allowed to change freely in a certain extent. Bag-of-Words, the method that is discussed in this paper, is another approach that can tolerate the flexibility in structures of the object and thus, have a wider variation of application in many problems. 

\subsection{Bag-of-Words}\label{section:background_bow}

Since Bag-of-Words is originally a text retrieval algorithm, we will first introduce some backgrounds about BoW in text retrieval problem in subsection \ref{section:bow_text} before discussing using BoW in image retrieval in subsection \ref{section:bow_image}.

\subsubsection{Bag-of-Words in text retrieval}\label{section:bow_text}
In text retrieval, a text is represented as a histogram of words, also known as BoW \cite{4}. This scheme is called term frequency weighting as the value of each histogram bin is equal to the number of times the word appears in the document.
Moreover, some words are less informative than others since those words appear in almost every document. Therefore, we need a weighting scheme that address this problem. Such weighting scheme is called inverse document frequency (idf) and is formulated as $log(N_{D} / N_{i})$, where $N_{D}$ is the number of documents in the collection and $N_{i}$ is the number of documents which contains word $i$. The overall BoW representation is thus weighted by multiplying the term frequency (tf) with the inverse document frequency (idf) giving rise to the tf-idf weighting \cite{4}. In addtion, extremely frequent words, ``stop words'', can be removed entirely in order to reduce storage requirements and query time.


\subsubsection{Bag-of-Words in image retrieval} \label{section:bow_image}

When applying BoW to image retrieval, a major obstacle is the fact that text documents are naturally broken into words by spaces, dots, hyphens, or commas. In contrast, there is no such separator in images. Therefore, the concept of ``visual word'' is introduced where each visual word is represented as a cluster obtained using k-means on the local descriptor vectors \cite{3}.

The bigger the vocabulary size is, the more different the visual words are. Hence, the vocabulary helps us distinguish the images more effectively. Nonetheless, with bigger vocabulary size, slightly different descriptors can be assigned to different visual words thus not contributing to the similarity of the respective images and causing a drop in performance examined in \cite{5, 6, 7}. Philbin et al. \cite{7} suggests ``soft assign'' method where each descriptor is assigned to multiple nearest visual words instead of using ``hard assignment'', i.e only assign a local descriptor to only one nearest visual word. Despite its effectiveness, this method also significantly costs more storage and time.


\subsection{Geometric transformation in images} \label{section:geometric_transformation}

\begin{figure}
    \centering
    \includegraphics[width=3.0in]{transformation.png}
    \caption{Transformation methods (Source: geodata.ethz.ch)}
    \label{fig:image_houses}
\end{figure}

In computer, an image ussually represents as a matrix of pixels. In other words, it is an array I which I(x, y) is value of the pixel at x and y in horizontal and vertical coordinates, respectively. A geometric transformation T is a rule or formular which transform every pair of (x, y) into (x', y'), i.e. $(x', y') = T(x, y)$. Based on how different a transformed image is, compared to the original one, geometric transformation is divided into many types (e.g. similarity, affine, projective, polinomial ...). In this section, the authors describe similarity, affine and projective transformations which are used in our framework.

\subsubsection{Similarity transformation} A similarity transformation is a transformation matrix T such that a new image A' is obtained by applying T into all pairs of (x, y) in A, i.e. $A'(x', y') = A(x, y)$. The main property of similarity transformation is that A and A' have the same shape. In other words, corresponding sides in A and A' are in proportion, corresponding angles in A and A' have the same measure. Similarity transformation includes scaling, translation, rotation, reflection and compositions of them in any combination and order.

\subsubsection{Affine transformation} Similar to similarity transformation, an affine transformation is also a matrix T that transform an image A into a new image A'. However, angles between lines as well as distances between points may not be preserved, though ratios of distances between points lying on a straight line are preserved. Affine transformation includes similarity transformation, homothety, shear mapping and compositions of them in any combination and order.

\subsubsection{Projective transformation} Generalized from affine transformation, projective transformation may not preserve the parallelism, i.e. two parallel lines/planes can be transformed into two intersecting lines/planes.

Because of the differential in complexity between transformations, the estimation process of different transformations requires different number of pairs $(x, y)$ and $(x', y')$. More specifically, Similarity, affine and projective transformations require at least 2, 3 and 4 pairs of coordinates, respectively.


\subsection{Evaluation the performance of image retrieval system} \label{section:evaluation_ir_sys}

Since there are many different algorithms lying behind different information retrieval systems, it is crucial to have a measurement for evaluating the performance of information retrieval systems. In this subsection, we will describe different popular measurements which are used to estimate performance of an information retrieval system. First of all, the authors would like to introduce 3 things that are required to evaluate information retrieval systems: a document collection, a set of queries, and a set of relevant documents for each query.

The 2 early born and fundamental measures are precision and recall. While precision is calculated as the ratio between the number of relevant documents that are retrieved and the total number of retrieved documents, recall is the quotient between the number of relevant documents retrieved and the number of relevant documents. The formula of these 2 measures are shown below:

\begin{equation}
recall = \frac{number\;of\;relevant\;items\;retrieved}{number\;of\;relevant\;items\;in\;collection}
\end{equation}

\begin{equation}
precision = \frac{number\;of\;relevant\;items\;retrieved}{total\;number\;of\;items\;retrieved}
\end{equation}

Despite their simplicity, precision and recall fail to take into account of the order of retrieved documents which is also important since a user want to find his/her desired document as fast as possible. Therefore, many other measures, which also consider the arrangement of the retrieved documents, are developed based on precision and recall. There is one such measure named Average Precision (AP), the average value of the precision value achieved from the top documents cut-off at positions where relevant documents are retrieved. In addition to precision, recall, and AP, there are also many other measures such as F-measure, i.e. weighted average of precision and recall or R-precision, i.e. the precision at R-th position where R is the number of relevant documents.

In our experiments, to evaluate the result, the authors choose to use the AP measure along with the mean value of AP overall queries, namely mean Average Precision (mAP). Besides the fact that these 2 measures can fully evaluate both the content and the order of the ranked list, their popularity are also a reason why the authors choose them. AP and mAP are also used in many previous works in image retrieval, so we can easily compare our framework's performance with others based on these 2 measures.
