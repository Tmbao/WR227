% \addbibresource{refs.bib}

\section{Method} \label{section:method}
\subsection{Overview of an Image Retrieval System}

As described in Figure \ref{fig:image_retrieval_system}, a typical Image Retrieval System consists of 2 main steps: Feature Extraction and Similarity Matching. The Feature Extraction step is explained in detail in section \ref{section:feature_extraction}. Subsequently, in section \ref{section:similarity_extraction}, the authors focus on describing how we perform Similarity Matching with our implementation of BoW model.


\subsection{Feature Extraction} \label{section:feature_extraction}

To detect and extract features from images, there are many methods that have been proposed (Harris-Affine, Hessian-Affine detectors \cite{Mikolajczyk2004}, Maximally stable extremal region (MSER) detector \cite{conf/bmvc/MatasCUP02}, Edge-based region detector \cite{Tuytelaars99content-basedimage}, Intensity extrema-based region detector \cite{Tuytelaars00widebaseline} ...). The authors choose to use Hessian-Affine detector, for detecting and extracting features from images. By using Hessian-Affine detector, which is also used in other baseline methods, our experiment's result can easily be compared with other ones.

As we tested on Oxford Building 5K Dataset \cite{oxbuilding}, there are typically 3,300 features for each image and a total about 16 millions of features for the whole dataset. Then, we compute the SIFT descriptor \cite{Lowe2004} of all the features and these descriptors is used for matching images in the next step.

\begin{figure}
    \centering
    \includegraphics[width=3.0in]{ImageRetrievalSystem.pdf}
    \caption{How an Image Retrieval System works}
    \label{fig:image_retrieval_system}
\end{figure}

\subsection{Similarity Matching} \label{section:similarity_extraction}

As described in Figure \ref{fig:proposed_framework}, our proposed framework would consist of the following major parts: Dictionary Building, Quantization, tf-idf Weighting and Spatial Rerank. The 3 former steps are described in section \ref{section:dictionary_building}, section \ref{section:quantization}, and section \ref{section:tfidf_weighting}. The last step is our main focus in this paper and is discussed in section \ref{section:spatial_rerank}.

\begin{figure}
    \centering
    \includegraphics[width=3.0in]{process.pdf}
    \caption{Proposed framework}
    \label{fig:proposed_framework}
\end{figure}

\subsubsection{Dictionary Building} \label{section:dictionary_building}
Treating each descriptor as an individual visual words in the dictionary results in a worthless waste of resources and time. In order to overcome this obstacle, the authors therefore build the dictionary by considering some similar descriptors as one. In other words, all descriptor vectors are divided into $k$ clusters, each representing a visual word. There are many algorithms that are proposed to solve this kind of problem. However, the authors use the approximate k-means (AKM). AKM is proposed by Philbin et al. \cite{2}. Comparing to the original k-means, AKM can reduce the majority amount of time taken by exact nearest neighbors computation but only gives slightly different result. Also, in \cite{2}, Philbin et al. shows that using 1M dictionary size would have the best performance on the Oxford Building 5K Dataset \cite{oxbuilding}.

\subsubsection{Quantization} \label{section:quantization}

Subsequently, each 128-dimension SIFT descriptor needs to be mapped into the dictionary. Commonly, each descriptor is assigned into the nearest word in the dictionary. Thus, when two descriptors are assigned to diferent words, they are considered as totally different. In practice, this hard assignment leads to errors due to variability in descriptor (e.g. image noise, varying scene illumination, instability in the feature detection process ...) \cite{7}. In order to handling this problem, the authors use soft assignment instead of hard assignment. In particular, each 128-dimension SIFT descriptor is reduced to a k-dimension vector of their $k$ nearest visual words in the dictionary. Each of these $k$ nearest cluster is assigned with weights calculated from the formula proposed by Sivic et al. \cite{7}, $weight = \exp(-\frac{d^2}{2\delta^2})$, where $d$ is the distance from the cluster center to descriptor point. Then, by adding all these weights to their corresponding bins, we will have the BoW representation of an image.

In this work, $k$ and $\delta^2$ are chosen to be 3 and 6250, respectively.

\subsubsection{tf-idf Weighting Scheme} \label{section:tfidf_weighting}

As mentioned in section \ref{section:background_relatedworks}, tf-idf is a popular weighting scheme that is used by almost any BoW model. In this section, the authors will show how this scheme is applied to our system.

For a term $t_{i}$ in a particular document $d_{j}$, its term frequency $tf_{i, j}$ is defined as follow:

\begin{equation} 
        tf_{i, j} = \frac{n_{i, j}}{\sum\limits_{k} n_{k, j}}
\end{equation}
Where $n_{i, j}$ is the number of occurrences of the considered term $t_{i}$ in the document $d_{j}$. The denominator is the sum of the number of occurrences of all the terms in document $d_{j}$.

The inverse document frequency $idf_{i}$ of a term $t_{i}$ is computed by the following formula:

\begin{equation}
        idf_{i} = \log{\frac{\left|D\right|}{\left|\{j: t_{i} \in d_{j}\}\right|}}
\end{equation}
Where, $\left|D\right|$ is the total number of documents in the corpus, $\left|\{j: t_{i} \in d_{j}\}\right|$ is the number of documents where the term $t_{i}$ appears, i.e. $n_{i, j} \ne 0$

The tf-idf weight of a term $t_{i}$ in a document $d_{j}$ is then calculated as the product of tf and idf:

\begin{equation}
{tfidf}_{i, j} = tf{i, j} \times idf_{i}
\end{equation}

The tf-idf weight is then used to compute the similarity score between an image $d_{i}$ and a query $q$:

\begin{equation}
s_{d_{i}, q} = \vec{{tfidf}_{i}} \cdot \vec{{tfidf}_{q}} = \sum\limits_{j = 1}^{\left|T\right|} {tfidf}_{i, j} \times {tfidf}_{q, j}
\end{equation} 

Finally, by sorting the list of images corresponding to their similarity score with a query, we achieve the raw ranked list of this query which is then used for the Spatial Rerank step.

\subsubsection{Spatial Rerank} \label{section:spatial_rerank}

\begin{figure}
    \centering
    \includegraphics[width=3.0in]{houses.png}
    \caption{Different images recorded from the same object under various viewpoints}
    \label{fig:image_houses}
\end{figure}

When applying BoW model into documents, we often ignore the spatial structure of words. However, the spatial structures of words in documents, especially images, are important for retrieving and ranking. Therefore, we push the spatial information of visual words into the original BoW model by incorporating the spatial constrains to the top ranked images and rerank them. 

The spatial verification process evaluates a geometric transformation of a image and the query. In detail, we need to estimate the geometric transformation matrix that transforms features of the query to features of a image. As described in Figure \ref{fig:image_houses}, objects can be taken under various viewpoints and it means the parallelism may not be preserved. Consequently, a projective transformation , which requires at least a set of 4 matched pairs of points to be estimated, is needed in the transformation matrix. A common approach is to use RANSAC \cite{Fischler1981}, i.e. choosing 4 matched pairs of points randomly to generate different transformation hypotheses multiple times and taking the hypotheses which has the largest number of \enquote{inliers}.

The spatial verification process evaluates the geometric transformation between the query image and each image in the top-$k$ ranked results. More specifically, given a query and an image, we need to estimate the geometric transformation matrix that transforms features of the query to features of the image. As shown in Figure \ref{fig:image_houses}, images of objects can be taken under various viewpoints and it means the parallelism may not be preserved. As a result, the computation of the projective transformation matrix \ref{section:geometric_transformation} requires a set of at least 4 matched pairs of feature points between the query and the image (a feature $x$ in the query and a feature $y$ in the image are called a matched pair if they belong to the same cluster of visual word). Additionally, the measurement of the transformation must also be taken into account, since we need to find a transformation that can accurately satisfies the spatial relation. Thus, the authors consider the number of \enquote{inliers} to be the measurement for a transformation. A matched pair $(x, y)$ is an \enquote{inlier} if apply the computed matrix on coordinate of feature $x$ would produce $x'$ such that $x'$ and $y$ are approximately in the same position. One common approach to find the needed transformation matrix is to apply RANSAC algorithm \cite{Fischler1981}, i.e. repeatedly choosing 4 matched pairs of points randomly to generate different transformation hypotheses, the hypothesis which has the highest measurement (in other words, the largest number of \enquote{inliers}) is the geometric transformation that we need. Finally, with this chosen geometric transformation, we then add the idf weight of the \enquote{inliers} to the new similarity score and rerank these top-$k$ ranked images. In figure \ref{fig:ransac}, we show an example image of RANSAC algorithm.

\begin{figure}
    \centering
    \includegraphics[width=3.0in]{ransac.jpg}
    \caption{Example of RANSAC algorithm. Verified matched features are colored green, unverified matched features are colored red}
    \label{fig:ransac}
\end{figure}

In our system, the authors choose the number of iterations for the RANSAC algorithm is 100 times and perform spatial rerank on the top 800 retrieved result of the dataset, which is showed to obtain the best accuracy by Philbin et al. \cite{2}.
